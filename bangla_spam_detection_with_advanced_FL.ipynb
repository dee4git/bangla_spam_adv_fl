{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d9adb6-a8b6-4b68-93ff-38dee7dd49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: {0, 1}\n",
      "Total samples: 5667\n",
      "Class distribution: {0: 4299, 1: 1368}\n",
      "\n",
      "Client Data Distributions:\n",
      "Client 0: 520 samples - Spam: 56.0%, Ham: 44.0%\n",
      "Client 1: 427 samples - Spam: 50.1%, Ham: 49.9%\n",
      "Client 2: 356 samples - Spam: 44.1%, Ham: 55.9%\n",
      "Client 3: 774 samples - Spam: 3.6%, Ham: 96.4%\n",
      "Client 4: 573 samples - Spam: 4.5%, Ham: 95.5%\n",
      "Client 5: 426 samples - Spam: 5.9%, Ham: 94.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Federated Learning simulation with 6 clients\n",
      "Running for 5 rounds with 1 epochs per round\n",
      "Device: cpu\n",
      "\n",
      "Round 1/5\n",
      "Training client 0...\n",
      "Client 0 - Loss: 0.6549\n",
      "Training client 1...\n",
      "Client 1 - Loss: 0.6554\n",
      "Training client 2...\n",
      "Client 2 - Loss: 0.6754\n",
      "Training client 3...\n",
      "Client 3 - Loss: 0.2440\n",
      "Training client 4...\n",
      "Client 4 - Loss: 0.2737\n",
      "Training client 5...\n",
      "Client 5 - Loss: 0.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1 - Evaluation Results:\n",
      "- Accuracy: 0.7584\n",
      "- Precision (Spam): 0.0000\n",
      "- Recall (Spam): 0.0000\n",
      "- F1 Score (Spam): 0.0000\n",
      "\n",
      "Round 2/5\n",
      "Training client 0...\n",
      "Client 0 - Loss: 0.7250\n",
      "Training client 1...\n",
      "Client 1 - Loss: 0.6689\n",
      "Training client 2...\n",
      "Client 2 - Loss: 0.7141\n",
      "Training client 3...\n",
      "Client 3 - Loss: 0.1701\n",
      "Training client 4...\n",
      "Client 4 - Loss: 0.2253\n",
      "Training client 5...\n",
      "Client 5 - Loss: 0.2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 2 - Evaluation Results:\n",
      "- Accuracy: 0.8034\n",
      "- Precision (Spam): 0.7339\n",
      "- Recall (Spam): 0.2920\n",
      "- F1 Score (Spam): 0.4178\n",
      "\n",
      "Round 3/5\n",
      "Training client 0...\n",
      "Client 0 - Loss: 0.7236\n",
      "Training client 1...\n",
      "Client 1 - Loss: 0.6659\n",
      "Training client 2...\n",
      "Client 2 - Loss: 0.6879\n",
      "Training client 3...\n",
      "Client 3 - Loss: 0.1804\n",
      "Training client 4...\n",
      "Client 4 - Loss: 0.1871\n",
      "Training client 5...\n",
      "Client 5 - Loss: 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 3 - Evaluation Results:\n",
      "- Accuracy: 0.7804\n",
      "- Precision (Spam): 0.6263\n",
      "- Recall (Spam): 0.2263\n",
      "- F1 Score (Spam): 0.3324\n",
      "\n",
      "Round 4/5\n",
      "Training client 0...\n",
      "Client 0 - Loss: 0.6535\n",
      "Training client 1...\n",
      "Client 1 - Loss: 0.6458\n",
      "Training client 2...\n",
      "Client 2 - Loss: 0.6986\n",
      "Training client 3...\n",
      "Client 3 - Loss: 0.1670\n",
      "Training client 4...\n",
      "Client 4 - Loss: 0.1848\n",
      "Training client 5...\n",
      "Client 5 - Loss: 0.2412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 4 - Evaluation Results:\n",
      "- Accuracy: 0.8236\n",
      "- Precision (Spam): 0.9625\n",
      "- Recall (Spam): 0.2810\n",
      "- F1 Score (Spam): 0.4350\n",
      "\n",
      "Round 5/5\n",
      "Training client 0...\n",
      "Client 0 - Loss: 0.6652\n",
      "Training client 1...\n",
      "Client 1 - Loss: 0.6886\n",
      "Training client 2...\n",
      "Client 2 - Loss: 0.6844\n",
      "Training client 3...\n",
      "Client 3 - Loss: 0.1660\n",
      "Training client 4...\n",
      "Client 4 - Loss: 0.1532\n",
      "Training client 5...\n",
      "Client 5 - Loss: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to confusion_matrix.png\n",
      "\n",
      "Misclassification Analysis:\n",
      "Showing 5 of 250 misclassified examples:\n",
      "\n",
      "Example 1:\n",
      "Text: nan\n",
      "True: Spam, Predicted: Ham\n",
      "\n",
      "Example 2:\n",
      "Text: nan\n",
      "True: Spam, Predicted: Ham\n",
      "\n",
      "Example 3:\n",
      "Text: \n",
      "বিষয়ঃ একমাত্র সফ্টওয়্যার যা আমরা প্রতিশ্রুতি দিতে পারি তা 100% বৈধ। খুব সস্তা দামে নাম-ব্র্যান্ড ...\n",
      "True: Spam, Predicted: Ham\n",
      "\n",
      "Example 4:\n",
      "Text: বিষয়: অংশীদারি\n",
      "মি. এডওয়ার্ড মোকো\n",
      "18 ইন্ডিপেন্ডেন্স ক্লোজ, জোহানেসবার্গ, দক্ষিণ আফ্রিকা।\n",
      "\n",
      "প্রিয় স্...\n",
      "True: Spam, Predicted: Ham\n",
      "\n",
      "Example 5:\n",
      "Text: ### ব্যবস্থাপনা:\n",
      "True: Spam, Predicted: Ham\n",
      "\n",
      "Round 5 - Evaluation Results:\n",
      "- Accuracy: 0.7795\n",
      "- Precision (Spam): 1.0000\n",
      "- Recall (Spam): 0.0876\n",
      "- F1 Score (Spam): 0.1611\n",
      "\n",
      "Simulation complete! Analyzing results...\n",
      "Performance metrics chart saved to metrics_over_rounds.png\n",
      "Loss and accuracy chart saved to loss_accuracy_over_rounds.png\n",
      "\n",
      "================================================================================\n",
      "                    FEDERATED LEARNING RESULTS SUMMARY\n",
      "================================================================================\n",
      "Round     Accuracy       Precision      Recall         F1 Score       \n",
      "----------------------------------------------------------------------\n",
      "1         0.7584          0.0000         0.0000         0.0000\n",
      "2         0.8034          0.7339         0.2920         0.4178\n",
      "3         0.7804          0.6263         0.2263         0.3324\n",
      "4         0.8236          0.9625         0.2810         0.4350\n",
      "5         0.7795          1.0000         0.0876         0.1611\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Final Model Performance Summary:\n",
      "- Accuracy: 0.7795\n",
      "- Precision (Spam): 1.0000\n",
      "- Recall (Spam): 0.0876\n",
      "- F1 Score (Spam): 0.1611\n",
      "\n",
      "Class-specific Metrics:\n",
      "\n",
      "Ham Class:\n",
      "- Precision: 0.7748\n",
      "- Recall: 1.0000\n",
      "- F1-Score: 0.8731\n",
      "\n",
      "Spam Class:\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.0876\n",
      "- F1-Score: 0.1611\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flwr as fl\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configuration dictionary\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"num_rounds\": 5,\n",
    "    \"num_clients\": 6,\n",
    "    \"epochs_per_round\": 1,\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_length\": 256,\n",
    "    \"model_name\": \"xlm-roberta-base\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"spam_ratio\": 0.8,  # For non-IID distribution\n",
    "    \"results_file\": \"federated_learning_results.json\",\n",
    "    \"confusion_matrix_file\": \"confusion_matrix.png\",\n",
    "    \"metrics_chart_file\": \"metrics_over_rounds.png\",\n",
    "    \"loss_accuracy_chart_file\": \"loss_accuracy_over_rounds.png\"\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset class\n",
    "class BanglaSpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])  # Ensure text is a string\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=CONFIG[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    # Google Sheet CSV export URL\n",
    "    sheet_id = \"1TL5gYe07k4ZF5HoLHia64xs5J6JMOPxCFQEVAzNseTw\"\n",
    "    tab_name = \"data\"\n",
    "    csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={tab_name}\"\n",
    "\n",
    "    # Load directly from Google Sheets\n",
    "    df = pd.read_csv(csv_url)\n",
    "\n",
    "    # Ensure correct column names\n",
    "    df.columns = [\"label\", \"text\"]\n",
    "\n",
    "    # Convert all texts to strings to avoid type issues\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    print(\"Label classes:\", set(labels))\n",
    "    print(f\"Total samples: {len(texts)}\")\n",
    "    print(f\"Class distribution: {pd.Series(labels).value_counts().to_dict()}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=CONFIG[\"test_size\"],\n",
    "        stratify=labels,\n",
    "        random_state=CONFIG[\"seed\"]\n",
    "    )\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# Create non-IID partitions\n",
    "def create_non_iid_clients(X_train, y_train):\n",
    "    spam_indices = np.where(np.array(y_train) == 1)[0]\n",
    "    ham_indices = np.where(np.array(y_train) == 0)[0]\n",
    "    clients_data = []\n",
    "    client_distributions = []\n",
    "\n",
    "    # Create spam-dominant clients\n",
    "    for i in range(CONFIG[\"num_clients\"] // 2):\n",
    "        spam_size = int(len(spam_indices) * CONFIG[\"spam_ratio\"] / (CONFIG[\"num_clients\"] // 2))\n",
    "        ham_size = int(len(ham_indices) * (1 - CONFIG[\"spam_ratio\"]) / (CONFIG[\"num_clients\"] // 2))\n",
    "\n",
    "        spam_samples = np.random.choice(spam_indices, spam_size, replace=False)\n",
    "        ham_samples = np.random.choice(ham_indices, ham_size, replace=False)\n",
    "\n",
    "        client_indices = np.concatenate([spam_samples, ham_samples])\n",
    "        clients_data.append(client_indices)\n",
    "        \n",
    "        # Store distribution info\n",
    "        client_labels = [y_train[idx] for idx in client_indices]\n",
    "        distribution = {\n",
    "            \"client_id\": i,\n",
    "            \"spam_percentage\": sum(client_labels) / len(client_labels) * 100,\n",
    "            \"ham_percentage\": (1 - sum(client_labels) / len(client_labels)) * 100,\n",
    "            \"total_samples\": len(client_indices)\n",
    "        }\n",
    "        client_distributions.append(distribution)\n",
    "        \n",
    "        spam_indices = np.setdiff1d(spam_indices, spam_samples)\n",
    "        ham_indices = np.setdiff1d(ham_indices, ham_samples)\n",
    "\n",
    "    # Create ham-dominant clients\n",
    "    for i in range(CONFIG[\"num_clients\"] // 2, CONFIG[\"num_clients\"]):\n",
    "        ham_size = int(len(ham_indices) * CONFIG[\"spam_ratio\"] / (CONFIG[\"num_clients\"] // 2))\n",
    "        spam_size = int(len(spam_indices) * (1 - CONFIG[\"spam_ratio\"]) / (CONFIG[\"num_clients\"] // 2))\n",
    "\n",
    "        ham_samples = np.random.choice(ham_indices, ham_size, replace=False)\n",
    "        spam_samples = np.random.choice(spam_indices, spam_size, replace=False)\n",
    "\n",
    "        client_indices = np.concatenate([ham_samples, spam_samples])\n",
    "        clients_data.append(client_indices)\n",
    "        \n",
    "        # Store distribution info\n",
    "        client_labels = [y_train[idx] for idx in client_indices]\n",
    "        distribution = {\n",
    "            \"client_id\": i,\n",
    "            \"spam_percentage\": sum(client_labels) / len(client_labels) * 100,\n",
    "            \"ham_percentage\": (1 - sum(client_labels) / len(client_labels)) * 100,\n",
    "            \"total_samples\": len(client_indices)\n",
    "        }\n",
    "        client_distributions.append(distribution)\n",
    "        \n",
    "        ham_indices = np.setdiff1d(ham_indices, ham_samples)\n",
    "        spam_indices = np.setdiff1d(spam_indices, spam_samples)\n",
    "\n",
    "    # Print client data distributions\n",
    "    print(\"\\nClient Data Distributions:\")\n",
    "    for dist in client_distributions:\n",
    "        print(f\"Client {dist['client_id']}: {dist['total_samples']} samples - \"\n",
    "              f\"Spam: {dist['spam_percentage']:.1f}%, Ham: {dist['ham_percentage']:.1f}%\")\n",
    "    \n",
    "    return clients_data\n",
    "\n",
    "# Modified Flower client class\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, train_loader, client_id):\n",
    "        self.train_loader = train_loader\n",
    "        self.client_id = client_id\n",
    "        self.model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "            CONFIG[\"model_name\"], num_labels=2\n",
    "        ).to(device)\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=CONFIG[\"learning_rate\"]\n",
    "        )\n",
    "        self.training_metrics = []\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "\n",
    "        epoch_metrics = {\"client_id\": self.client_id, \"loss\": 0, \"samples\": 0}\n",
    "        \n",
    "        for epoch in range(CONFIG[\"epochs_per_round\"]):\n",
    "            running_loss = 0.0\n",
    "            samples = 0\n",
    "            \n",
    "            for batch in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "                samples += labels.size(0)\n",
    "\n",
    "            # Record metrics for this epoch\n",
    "            epoch_metrics[\"loss\"] = running_loss / samples\n",
    "            epoch_metrics[\"samples\"] = samples\n",
    "            \n",
    "        self.training_metrics.append(epoch_metrics)\n",
    "        print(f\"Client {self.client_id} - Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        \n",
    "        return self.get_parameters({}), len(self.train_loader.dataset), {\"client_id\": self.client_id, \"loss\": epoch_metrics[\"loss\"]}\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# Evaluation function with detailed metrics\n",
    "def evaluate_model(model, data_loader, return_predictions=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    if return_predictions:\n",
    "        return accuracy, all_preds, all_labels\n",
    "    return accuracy\n",
    "\n",
    "# Function to generate and save confusion matrix\n",
    "def generate_confusion_matrix(y_true, y_pred, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Ham (0)', 'Spam (1)'], \n",
    "                yticklabels=['Ham (0)', 'Spam (1)'])\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.close()  # Close the figure to prevent display in non-interactive mode\n",
    "    \n",
    "    # Calculate metrics from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        \"true_negative\": int(tn),\n",
    "        \"false_positive\": int(fp),\n",
    "        \"false_negative\": int(fn),\n",
    "        \"true_positive\": int(tp),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"accuracy\": float((tp + tn) / (tp + tn + fp + fn))\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Server-side evaluation with detailed metrics\n",
    "def server_evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    "    test_loader,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    metrics_history\n",
    "):\n",
    "    \"\"\"Evaluate model parameters on the test dataset.\"\"\"\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        CONFIG[\"model_name\"], num_labels=2\n",
    "    ).to(device)\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = {k: torch.tensor(v) for k, v in params_dict}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get accuracy and predictions\n",
    "    accuracy, all_preds, all_labels = evaluate_model(model, test_loader, return_predictions=True)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Ham', 'Spam'], output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Generate confusion matrix and metrics - only save on final round\n",
    "    cm_metrics = generate_confusion_matrix(\n",
    "        all_labels, all_preds, \n",
    "        save_path=CONFIG[\"confusion_matrix_file\"] if server_round == CONFIG[\"num_rounds\"] else None\n",
    "    )\n",
    "        \n",
    "    # Store metrics for this round\n",
    "    round_metrics = {\n",
    "        \"round\": server_round,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confusion_matrix_metrics\": cm_metrics,\n",
    "        \"classification_report\": report\n",
    "    }\n",
    "    metrics_history.append(round_metrics)\n",
    "    \n",
    "    # Save metrics history to file\n",
    "    with open(CONFIG[\"results_file\"], 'w') as f:\n",
    "        json.dump(metrics_history, f, indent=2)\n",
    "    \n",
    "    # Advanced error analysis on misclassifications - final round only\n",
    "    if server_round == CONFIG[\"num_rounds\"]:\n",
    "        misclassified_indices = [i for i, (true, pred) in enumerate(zip(all_labels, all_preds)) if true != pred]\n",
    "        num_samples = min(5, len(misclassified_indices))\n",
    "        \n",
    "        if num_samples > 0:\n",
    "            print(\"\\nMisclassification Analysis:\")\n",
    "            print(f\"Showing {num_samples} of {len(misclassified_indices)} misclassified examples:\")\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                idx = misclassified_indices[i]\n",
    "                # Make sure text is a string and handle potential type issues\n",
    "                text = str(X_test[idx])\n",
    "                text_display = text[:100] + \"...\" if len(text) > 100 else text\n",
    "                true_label = \"Spam\" if all_labels[idx] == 1 else \"Ham\"\n",
    "                pred_label = \"Spam\" if all_preds[idx] == 1 else \"Ham\"\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Text: {text_display}\")\n",
    "                print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "    \n",
    "    # Print round summary\n",
    "    print(f\"\\nRound {server_round} - Evaluation Results:\")\n",
    "    print(f\"- Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"- Precision (Spam): {cm_metrics['precision']:.4f}\")\n",
    "    print(f\"- Recall (Spam): {cm_metrics['recall']:.4f}\")\n",
    "    print(f\"- F1 Score (Spam): {cm_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Function to analyze and visualize results\n",
    "def analyze_results(client_metrics=None, results_file=CONFIG[\"results_file\"]):\n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            \n",
    "        # Plot metrics across rounds\n",
    "        rounds = [r[\"round\"] for r in results]\n",
    "        accuracy = [r[\"accuracy\"] for r in results]\n",
    "        precision = [r[\"confusion_matrix_metrics\"][\"precision\"] for r in results]\n",
    "        recall = [r[\"confusion_matrix_metrics\"][\"recall\"] for r in results]\n",
    "        f1 = [r[\"confusion_matrix_metrics\"][\"f1_score\"] for r in results]\n",
    "        \n",
    "        # Create the metrics chart\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(rounds, accuracy, 'o-', label='Accuracy')\n",
    "        plt.plot(rounds, precision, 's-', label='Precision')\n",
    "        plt.plot(rounds, recall, '^-', label='Recall')\n",
    "        plt.plot(rounds, f1, 'D-', label='F1 Score')\n",
    "        \n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Federated Learning Performance Metrics')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(CONFIG['metrics_chart_file'])\n",
    "        plt.close()\n",
    "        print(f\"Performance metrics chart saved to {CONFIG['metrics_chart_file']}\")\n",
    "        \n",
    "        # Create a loss and accuracy chart if client metrics are available\n",
    "        if client_metrics:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Create a subplot for loss\n",
    "            plt.subplot(1, 2, 1)\n",
    "            for client_id, metrics in client_metrics.items():\n",
    "                rounds = list(range(1, len(metrics) + 1))\n",
    "                loss_values = [m.get('loss', 0) for m in metrics]\n",
    "                plt.plot(rounds, loss_values, 'o-', label=f'Client {client_id}')\n",
    "            \n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss by Client')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Create a subplot for accuracy\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(rounds, accuracy, 'D-', linewidth=2, color='black', label='Global Model')\n",
    "            \n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Global Model Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(CONFIG['loss_accuracy_chart_file'])\n",
    "            plt.close()\n",
    "            print(f\"Loss and accuracy chart saved to {CONFIG['loss_accuracy_chart_file']}\")\n",
    "        \n",
    "        # Generate the results table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \" * 20 + \"FEDERATED LEARNING RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'Round':<10}{'Accuracy':<15}{'Precision':<15}{'Recall':<15}{'F1 Score':<15}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Data rows\n",
    "        for r in results:\n",
    "            print(f\"{r['round']:<10}{r['accuracy']:.4f}{'':10}{r['confusion_matrix_metrics']['precision']:.4f}{'':9}{r['confusion_matrix_metrics']['recall']:.4f}{'':9}{r['confusion_matrix_metrics']['f1_score']:.4f}\")\n",
    "        \n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Print final round summary\n",
    "        final_round = results[-1]\n",
    "        print(\"\\nFinal Model Performance Summary:\")\n",
    "        print(f\"- Accuracy: {final_round['accuracy']:.4f}\")\n",
    "        print(f\"- Precision (Spam): {final_round['confusion_matrix_metrics']['precision']:.4f}\")\n",
    "        print(f\"- Recall (Spam): {final_round['confusion_matrix_metrics']['recall']:.4f}\")\n",
    "        print(f\"- F1 Score (Spam): {final_round['confusion_matrix_metrics']['f1_score']:.4f}\")\n",
    "        \n",
    "        # Class-specific metrics\n",
    "        print(\"\\nClass-specific Metrics:\")\n",
    "        for cls in ['Ham', 'Spam']:\n",
    "            print(f\"\\n{cls} Class:\")\n",
    "            print(f\"- Precision: {final_round['classification_report'][cls]['precision']:.4f}\")\n",
    "            print(f\"- Recall: {final_round['classification_report'][cls]['recall']:.4f}\") \n",
    "            print(f\"- F1-Score: {final_round['classification_report'][cls]['f1-score']:.4f}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Results file {results_file} not found. Run the federated learning simulation first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing results: {e}\")\n",
    "\n",
    "# Modified implementation to avoid Ray transport errors\n",
    "def run_federated_learning_simulation():\n",
    "    # Clear previous results if they exist\n",
    "    if os.path.exists(CONFIG[\"results_file\"]):\n",
    "        os.remove(CONFIG[\"results_file\"])\n",
    "    \n",
    "    # Load and prepare data\n",
    "    (X_train, y_train), (X_test, y_test) = load_data()\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "    # Create test loader\n",
    "    test_dataset = BanglaSpamDataset(X_test, y_test, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Create non-IID clients\n",
    "    client_indices = create_non_iid_clients(X_train, y_train)\n",
    "    \n",
    "    # Initialize clients\n",
    "    clients = []\n",
    "    for i, indices in enumerate(client_indices):\n",
    "        client_texts = [X_train[idx] for idx in indices]\n",
    "        client_labels = [y_train[idx] for idx in indices]\n",
    "        dataset = BanglaSpamDataset(client_texts, client_labels, tokenizer)\n",
    "        loader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "        clients.append(FlowerClient(loader, i))\n",
    "    \n",
    "    # Initialize server model\n",
    "    server_model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        CONFIG[\"model_name\"], num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get initial parameters\n",
    "    server_parameters = [val.cpu().numpy() for _, val in server_model.state_dict().items()]\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics_history = []\n",
    "    client_metrics = defaultdict(list)\n",
    "    \n",
    "    # Simulation loop\n",
    "    print(f\"\\nStarting Federated Learning simulation with {CONFIG['num_clients']} clients\")\n",
    "    print(f\"Running for {CONFIG['num_rounds']} rounds with {CONFIG['epochs_per_round']} epochs per round\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    for server_round in range(1, CONFIG[\"num_rounds\"] + 1):\n",
    "        print(f\"\\nRound {server_round}/{CONFIG['num_rounds']}\")\n",
    "        \n",
    "        # Client-side training\n",
    "        client_results = []\n",
    "        for client_idx, client in enumerate(clients):\n",
    "            print(f\"Training client {client_idx}...\")\n",
    "            parameters, num_examples, client_metrics_dict = client.fit(server_parameters, {})\n",
    "            client_results.append((parameters, num_examples))\n",
    "            \n",
    "            # Store client metrics\n",
    "            client_metrics[client_idx].append(client_metrics_dict)\n",
    "        \n",
    "        # Aggregate parameters (FedAvg)\n",
    "        if client_results:\n",
    "            weights = [num_examples for _, num_examples in client_results]\n",
    "            total_examples = sum(weights)\n",
    "            weighted_parameters = [\n",
    "                [layer * weight / total_examples for layer in parameters]\n",
    "                for parameters, weight in zip(\n",
    "                    [parameters for parameters, _ in client_results], weights\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sum up weighted parameters\n",
    "            server_parameters = [\n",
    "                np.sum(\n",
    "                    [params[i] for params in weighted_parameters], axis=0\n",
    "                )\n",
    "                for i in range(len(weighted_parameters[0]))\n",
    "            ]\n",
    "        \n",
    "        # Server-side evaluation\n",
    "        accuracy = server_evaluate(\n",
    "            server_round=server_round,\n",
    "            parameters=server_parameters,\n",
    "            config={},\n",
    "            test_loader=test_loader,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            metrics_history=metrics_history\n",
    "        )\n",
    "        \n",
    "    # Final analysis\n",
    "    print(\"\\nSimulation complete! Analyzing results...\")\n",
    "    analyze_results(client_metrics=client_metrics)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    run_federated_learning_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cfc328-510f-4c66-934c-08814b78e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_accuracy(metrics_history, client_metrics, config=CONFIG):\n",
    "    # Gather rounds\n",
    "    rounds = [r['round'] for r in metrics_history]\n",
    "    # Global accuracy per round (already collected)\n",
    "    accuracy = [r['accuracy'] for r in metrics_history]\n",
    "    # Average client loss per round\n",
    "    avg_client_loss = []\n",
    "    num_clients = max(client_metrics.keys()) + 1\n",
    "\n",
    "    # Gather average loss for each round across all clients\n",
    "    for rnd in range(len(rounds)):\n",
    "        losses = []\n",
    "        for client_id in client_metrics:\n",
    "            if len(client_metrics[client_id]) > rnd:\n",
    "                losses.append(client_metrics[client_id][rnd]['loss'])\n",
    "        avg_loss = np.mean(losses) if losses else 0\n",
    "        avg_client_loss.append(avg_loss)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(rounds, accuracy, 'o-', label=\"Global Test Accuracy\")\n",
    "    plt.plot(rounds, avg_client_loss, 's-', label=\"Average Client Training Loss\")\n",
    "    plt.xlabel('Round')\n",
    "    plt.title(\"Federated Learning: Test Accuracy & Avg Client Loss per Round\")\n",
    "    plt.ylabel('Score / Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config['loss_accuracy_chart_file'])\n",
    "    plt.show()\n",
    "    print(f\"Loss and accuracy plot saved to {config['loss_accuracy_chart_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8327ff-6b41-4685-986f-e00c6e2e1d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
